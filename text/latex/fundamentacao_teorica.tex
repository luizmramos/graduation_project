\section{Definição formal}
Utilizando a notação adotada por Dan Jurafsky e Christopher Manning em seu curso de Processamento de Linguagem Natural para Stanford \cite{text_classification}, define-se o problema da classificação supervisionada de textos da seguinte forma.

Seja $C=\{c_1, c_2, ..., c_J\}$ um conjunto fixo de classes, $D=\{d_1, d_2, ..., d_n\}$ um conjunto de documentos, e $\mathcal{T}=\{(d_1, c_{d_1}), (d_2, c_{d_2}), ... , (d_m, c_{d_m})\}$ um conjunto de treinamento (subconjunto de $D$) com $m$ documentos classificados manualmente, o classificador consiste em uma função $\gamma :D\rightarrow C$ que relaciona um documento a uma classe, e um algoritmo de aprendizado de máquina supervisionado é um algoritmo que recebe como parâmetros $C$ e $\mathcal{T}$ e retorna $\gamma$.

\section{Tipos de classificadores}
Existem diversos tipos diferentes de classificadores que possuem resultados muito bons dependendo do problema analisado. Segue abaixo uma lista dos principais classificadores existentes.

\begin{itemize}	
	\item Árvores de Decisão
	\item Naïve Bayes
	\item Regressão Logística
	\item Support Vector Machines
	\item k-Nearest Neightbors
	\item Redes Neurais
	\item Dentre outros
\end{itemize}

A performance de todos estes métodos varia consideravelmente dependendo da aplicação. Estudos mostram que para bases de dados grandes o suficiente, ótimos resultados podem ser atingidos independentemente do método utilizado \cite{practical_issues}. Entretanto, para uma quantidade pequena de dados as Naïve Bayes apresentam resultados bons por serem classificadores de alto \emph{bias} / baixa variância \cite{quora_classifier}. 

As NB possuem as vantagens de serem fáceis de se implementar, serem bem rápidas na hora da execução e mostrarem bons resultados práticos.

\section{Redes Bayesianas}

\subsection{Definição}
Em modelagem gráfica probabilística, Redes Bayesianas são grafos direcionados que representam relações de dependência condicionais entre diferentes variáveis aleatórias \cite{introduction_to_graphical_models}. A partir da visualização de uma Rede Bayesiana é possível utilizar a regra de Bayes para realizar inferências e descobrir a probabilidade de eventos, dadas algumas variáveis observadas.As arestas direcionadas representam as noções de causalidade entre as variáveis aleatórias e geram as dependências condicionais. Para cada nó do gráfico deve haver uma tabela de probabilidades condicionais para a variável em questão.

A Figura \ref{fig:bayesian_networks}, retirada do curso de Modelagem Gráfica Probabilística da professora Daphne Koller de Stanford \cite{probabilistic_graphical_models}, ilustra um exemplo de uma Rede Bayesiana simples. Neste caso, pode-se ver que a nota do aluno é influenciada pela dificuldade da prova e pela sua inteligencia. Alem disso A possibilidade do professor escrever uma carta de recomendação depende apenas da nota do aluno e o SAT do aluno depende apenas de sua inteligencia. Em cada nó do grafo há uma tabela de distribuições de probabilidades condicionais.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{bayesian_networks.png}
	\caption{Exemplo de Redes Bayesianas}
	\label{fig:bayesian_networks}
\end{figure}


\subsection{Regra de Bayes}
Para realizar inferências nas redes Bayesianas utiliza-se a regra de Bayes, como definida a seguir.

Sejam $A$ e $B$ dois eventos com probabilidades de ocorrência $P(A)$ e $P(B)$ e sendo $P(A|B)$ e $P(B|A)$ as probabilidades condicionais de $A$ dado $B$ e de $B$ dado $A$, respectivamente, tem-se que:

$P(A|B) = \frac{{P(A) P(B|A)}}{P(B)}$

Esse resultado parte da noção probabilidade conjunta $P(A,B)$.

$P(A,B)=P(A) P(B|A) = P(B) P(A|B) \rightarrow P(A|B) = \frac{{P(A) P(B|A)}}{P(B)}$

No caso do exemplo da Figura \ref{fig:bayesian_networks}, podemos calcular a probabilidade conjunta da rede da seguinte forma:

$P(D,I,G,S,L)=P(D)P(I)P(G|I,D)P(S|I)P(L|G)$ 

Onde $D=Difficulty$, $G=Grade$, $I=Intelligence$, $S=SAT$ e $L=Letter$.

\subsection{Naïve Bayes}
\subsubsection{Problema a ser resolvido}
Redes Bayesianas são ferramentas excelentes para modelar problemas complexos, entretanto elas possuem um grande problema prático. A realização de inferências em redes genéricas é um problema NP-Hard, conforme demonstrado por Cooper \cite{inference_bayesian_networks}.

O que é realizado na prática é, ou realizar inferências aproximadas nas redes com algoritmos polinomiais, ou simplificar as redes a alguns tipos específicos mais simples.

\subsubsection{O que são Naïve Bayes}
As Naïve Bayes (bayes ingênuas) são simplificações feitas na modelagem de um problema por redes Bayesianas de modo a tornar possível realizar a inferência de forma rápida. Elas assumem que as variáveis do problema são condicionalmente independentes (mesmo que na prática elas não sejam, o que explica o nome \emph{ingênuas}).

A Figura \ref{fig:naive_bayes_example} mostra um exemplo de uma NB comum. Ela possui uma variável $Classe$ que depende de um série de outras varáveis $x_1, x_2, x_3, ... , x_n$ (que serão chamadas a partir daqui de features). É importante notar que a estrutura da rede mostra que as features são condicionalmente independentes umas das outras e a $Classe$ depende de cada uma das features individualmente. É fácil entender a grande vantagem desta abordagem. Cada tabela de probabilidades condicionais será pequena. Alem disso a inferência da variável classe, dadas algumas das features será bem simples, como será mostrado nas próximas seções.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{naive_bayes_example.png}
	\caption{Exemplo de uma Naïve Bayes}
	\label{fig:naive_bayes_example}
\end{figure}

Na prática o NB é um ótimo classificador de textos, pois, além de ser simples, traz resultados comparáveis a outros classificadores substancialmente mais complexos e lentos.

\subsubsection{Independência Condicional}
Antes de explicar a inferência em NB é importante entender o que significa o conceito de independência condicional.

Dados dois eventos $A$ e $B$, dizemos que eles são independentes se a probabilidade de ocorrência de um deles não é influenciada pelo fato do outro ter ocorrido. Ou seja:

$P(A|B) = P(A)$
$P(B|A) = P(B)$

Esta propriedade é muito relevante na simplificação de expressões pois podemos usar o fato de que a probabilidade conjunta é igual ao produto das probabilidades individuais.

$P(A,B) = P(A)P(B|A) = P(A)P(B)$

\subsubsection{Inferência}
A regra de Bayes seguida pela propriedade de independência condicional pode ser utilizada para a realização de inferência em NB da seguinte forma.

Sejam $C=\{c_1,c_2,c_3, ..., c_J\}$ um conjunto de classes e $x_1, x_2, x_3, ..., x_n$ as features a serem analisadas. Como trata-se de NB, assume-se independência condicional das features. Deseja-se saber para cada $i$:

$P(C=c_i|x_1,x_2,...,x_n)$

Ou seja, deseja-se saber qual a probabilidade da classe possuir o valor $c_i$, dadas as features observadas.

Pela regra de Bayes temos:

\begin{equation}
P(C=c_i|x_1,x_2,...,x_n) = \frac{P(x_1,x_2,...,x_n|C=c_i)P(C=c_i)}{P(x_1,x_2,...,x_n)}
\label{eq:bayes_inference}
\end{equation}

Como $x_1, x_2, x_3, ..., x_n$ são condicionalmente independentes entre si, temos:


\begin{equation}
\begin{split}
P(x_1,x_2,...,x_n|C=c_i) &= P(x_1,x_2,...,x_{n-1}|x_n,C=c_i)P(x_n|C=c_i) \\ 
&= P(x_1,x_2,...,x_{n-1}|C=c_i)P(x_n|C=c_i) \\
&= P(x_1,x_2,...,x_{n-2}|x_{n-1},C=c_i)P(x_{n-1}|C=c_i)P(x_n|C=c_i) \\
&= P(x_1,x_2,...,x_{n-2}|C=c_i)P(x_{n-1}|C=c_i)P(x_n|C=c_i) \\
&= ... = P(x_1|C=c_i)P(x_2|C=c_i)P(x_3|C=c_i)...P(x_n|C=c_i)  \\
&= \prod_{j=1}^{n}P(x_j|C=c_i)
\end{split}
\label{eq:produtorio}
\end{equation}


Substituindo \ref{eq:produtorio} em \ref{eq:bayes_inference} temos:

\begin{equation}
P(C=c_i|x_1,x_2,...,x_n) = \frac{P(C=c_i)({\prod_{j=1}^{n}P(x_j|C=c_i)})}{P(x_1,x_2,...,x_n)}
\label{eq:naive_bayes_probability}
\end{equation}

\subsubsection{Classificação utilizando Naïve Bayes}
No problema de classificação temos um documento $d$ que será representado pelas features $x_1,x_2,...,x_n$ e deseja-se saber de qual classe $c \in C$ este documento pertence.

Matematicamente, deseja-se saber de qual classe é mais provável que o documento pertença. Isto é, a classe que maximiza $P(d|c)$.

$\gamma(d)=argmax_c(P(d|c))=argmax_c(P(x_1,x_2,...,x_n|c))$

Pela equação \ref{eq:naive_bayes_probability}, temos que:

$argmax_c(P(x_1,x_2,...,x_n|c)) = argmax_c(\frac{P(C=c_i)({\prod_{j=1}^{n}P(x_j|C=c_i)})}{P(x_1,x_2,...,x_n)})$

Como $P(x_1,x_2,...,x_n)$ não depende de c, pode-se cortá-lo do denominador, chegando em:

$\gamma(d) = argmax_c(P(C=c_i)({\prod_{j=1}^{n}P(x_j|C=c_i)})$

Agora, para descobrir a classe do documento, basta calcular $P(C=c_i)$ e $P(x_k|C=c_i)$ e ambas estas probabilidades são fáceis de serem estimadas (se tivermos um conjunto de treino suficientemente grande).

Assumindo que as features sejam binarias, isto é, ou estão presentes no documento, ou não estão. Seja $\#(x)$ um operador que indica quantas vezes a feature x aparece no conjunto de treinamento e $\#(c)$ quantos documentos do conjunto de treinamento possuem a classe c. Além disso, seja $\#(x \wedge c)$ a quantidade de vezes que a feature x aparece num documento de classe c e seja N o total de documentos do conjunto de treinamento. É fácil ver que:

$P(C=c_i) \simeq \frac{\#(c_i)}{N}$

e

$P(x_j|C=c_i) \simeq \frac{\#(x_j \wedge c_i)}{\#(c_i)}$

Ou seja, o problema de classificação se tornou basicamente um problema de contagem.

\subsubsection{Smoothing}
Um dos problemas práticos encontrados por NB é a ocorrência de contagens nulas. O grande problema do 0 é que se ele for apenas um dos fatores da multiplicação o resultado inteiro será 0, inutilizando o método.

Isto não é algo incomum, principalmente se o conjunto de treinamento for pequeno. Basta ter uma palavra no conjunto de teste que nunca ocorreu em uma determinada classe no conjunto de treinamento.

Existem técnicas que são utilizadas para resolver este problema e elas são chamadas de Smoothing. Neste trabalho utilizou-se uma das mais comuns: o Laplace Smoothing.

O Laplace consiste em assumir que todas as features foram vistas pelo menos $\alpha$ vezes em cada uma das classes. Isso se traduz nas seguintes formulas, sendo $L$ o número total de classes e $V$ o numero total de features.

$P(C=c_i) \simeq \frac{\#(c_i) + \alpha}{N + \alpha L}$

e

$P(x_j|C=c_i) \simeq \frac{\#(x_j \wedge c_i) + \alpha}{\#(c_i) + \alpha V}$

No caso especial em que $\alpha = 1$, tem-se o \emph{Add-One Smoothing}.

\subsection{Modelagens para classificação de texto}
Uma vez que já foi entendida a forma de classificar o texto, resta apenas representá-lo por um conjunto de features.

\subsubsection{Naïve Bayes Binária}
Uma forma comum de representar um texto em features é considerá-lo como um conjunto de palavras. Assume-se que cada palavra é uma feature que pode estar presente ou não no documento. Nesta representação a ordem das palavras não é importante.

Esta representação não leva em consideração a frequência com a qual cada palavra aparece no texto. A fórmula para calcular a classe mais provável é aquela que foi desenvolvida acima.

% TODO: Continuar essa explicação

\subsubsection{Naïve Bayes de Bernoulli}

\subsubsection{Multinomial Naïve Bayes}
É interessante levar em consideração a frequência da ocorrência das palavras uma vez que esta pode trazer informação relevante sobre a classe.
% TODO: Entender multinomial bayes e continuar


\subsubsection{Outras features}
É possível enriquecer o classificador utilizando outras features (que não sejam as próprias palavras do texto). Exemplos de features que podem ser utilizadas são: combinações de palavras, o tamanho do texto, quantidade de sinais de pontuação, quantidade de palavras com iniciais maiúsculas, etc.

Para o caso específico de postagens em redes sociais existem ainda outras features que podem ser incluídas. Pode-se considerar o autor da postagem, o momento em que ela foi publicada, a presença de fotos, vídeos ou links, etc.

\section{Weighted Naïve Bayes}
Um dos problemas das NB é que muitas vezes nas aplicações reais não é possível assumir a independência condicional das features. Muitas vezes uma das features tem um peso maior que as outras, por exemplo.

Um modo inicial de relaxar essa hipótese de independência, é eliminar features com alta correlação, fazendo com que o subconjunto restante se encaixe melhor na hipótese de independência condicional. Isto é chamado de seleção de features.

Neste caso temos:

$\gamma(d) = argmax_c(P(C=c_i)({\prod_{j=1}^{n}P(x_j|C=c_i)^{I(j)}})$

Onde:

$I(j) \in \{0,1\}$

Uma abordagem mais genérica é ponderar cada feature de acordo com sua relevância. Ou seja:

$\gamma(d) = argmax_c(P(C=c_i)({\prod_{j=1}^{n}P(x_j|C=c_i)^{w(j)}})$

Onde:

$w(j) \in \mathbb{R}^+$

Nota-se que a seleção de features é um caso específico da ponderação de features (onde $w(j)$ só pode ser 0 ou 1).

Agora o grande problema passa a ser determinar os pesos $w(j)$ das features. Há diversos algoritmos que ja foram propostos para realizar esta tarefa.

% TODO: Escrever sobre outros approachs the feature weighting (tem naquele artigo)

Neste trabalho, estudou-se a utilização de um método baseado na Teoria da Informação, que além de ser comum na literatura, possui fundamentos teóricos bem embasados \cite{weighted_naive_bayes}. 

% TODO: Escrever sobre feature weighting


\section{Métodos de avaliação de um classificador}
Uma vez desenvolvido o classificador é importante ser capaz de avaliá-lo a fim de determinar o quão bom ele é. A utilização de métricas numéricas para avaliar os classificadores é interessante pois permite a realização de comparações entre as diferentes versões implementadas, tornando possível determinar se as modificações que foram feitas estão fazendo efeito.

Existem diversas métricas que podem ser utilizadas para avaliar classificadores, algumas delas serão analisadas nesta seção \cite{sokolova2009systematic}.


\subsection{Classificador Binário}
Primeiro serão explorados os métodos de avaliação da performance do classificador binário, que é o classificador mais simples. 

A tabela \ref{tab:avaliacao_classificador_binario} resume as diferentes medidas que podem ser utilizadas, onde $tp$ é o verdadeiro positivo, $tn$ é o verdadeiro negativo, $fp$ é o falso positivo e $fn$ é o falso negativo.


\begin{table}[tph]
	\begin{center}
		\begin{tabular}{c c c}
			\hline
			Medida & Fórmula & Intuição \\
			\hline 
			&&\\
			Acurácia & $\frac{tp+tn}{tp+fn+fp+tn}$ & Eficácia global do classificador \\[0.6cm]
			Precisão & $\frac{tp}{tp+fp}$ & Quantos dos objetos classificados como positivo são efetivamente positivos\\[0.6cm]
			Abrangência & $\frac{tp}{tp+fn}$ & Quantos dos objetos positivos foram classificados como positivos\\[0.6cm]
			Fscore & $\frac{(\beta^2+1)tp}{(\beta^2+1)tp + \beta^2fn + fp}$ & Uma média harmônica entre a precisão e a abrangência \\[0.2cm]
			\hline
		\end{tabular}
	\end{center}
	\caption{Resumo das medidas de performance para classificadores binários}
	\label{tab:avaliacao_classificador_binario}
\end{table}

\subsubsection{Acurácia}
A acurácia mede a quantidade total de acertos (verdadeiro positivos ou verdadeiro negativos) em relação à quantidade total de objetos classificados.

$acuracia = \frac{tp+tn}{tp+fn+fp+tn}$

Esta é a medida de performance mais fácil de se entender, entretanto ela não é muito informativa, uma vez que dependendo da distribuição das classes, classificadores ruins podem ter alta acurácia.

Suponha, por exemplo, um classificador que classifica tumores em benignos ou malignos. Suponha também que 99.9\% dos tumores sejam benignos. É possível criar um classificador que sempre responde que o tumor é benigno e ele teria uma acurácia de 99.9\% (aparentemente muito boa). Entretanto este classificador falha miseravelmente na sua tarefa principal (que é determinar quais tumores são malignos para que as devidas providencias sejam tomadas).

\subsubsection{Precisão}
A precisão é uma estatística que indica quantos dos objetos classificados como positivo são efetivamente positivos. Quando um classificador com alto precisão classifica um objeto como positivo, é muito provável que ele seja positivo (entretanto nada se sabe sobre quantos objetos positivos ele esta classificando como negativo).

$precisao = \frac{tp}{tp+fp}$

Um classificador que indica para uma pessoa se ela deve investir ou não em uma determinada ação deve ser muito preciso. Pois para o investidor o importante é que quando ele siga o conselho do classificador e invista, ele não perca dinheiro. Não importa tanto se havia várias outras ações que eram boas, mas que o classificador desprezou.

\subsubsection{Abrangência (\emph{Recall})}
A abrangência de um classificador indica quantos dos objetos positivos são efetivamente classificados como positivos. Um classificador com grande abrangência provavelmente irá detectar a maior parte dos objetos positivos (apesar de também poder detectar alguns negativos como sendo positivos).

$abrangencia = \frac{tp}{tp+fn}$

Um médico que classifica pintas na pele das pessoas entre malignas ou benignas (câncer de pele ou não) deve ter uma grande abrangência (sendo a classificação 'maligno' a classe positiva do classificador), mas a precisão não é tão importante. Isso ocorre pois se o médico deixar de classificar alguma pinta que era maligna como maligna, o paciente pode acabar evoluindo a doença e morrer (ou seja, todos que são efetivamente malignos devem ser classificados como malignos). A precisão não é tão importante pois se por ventura o médico classificar uma pinta benigna como maligna, o paciente vai remover a mesma e na biópsia será possível identificar que não era nada demais (o paciente só terá que passar inutilmente pelo procedimento cirúrgico de remoção de pintas, mas isso não é muito problemático).

Existe, normalmente uma relação inversa entre a precisão e a abrangência. Em geral quanto se aumenta a precisão se reduz a abrangência e vice-versa. Pode-se exemplificar isto com o mesmo caso do médico que classifica pintas. Quando ele fica com dúvida se a pinta é benigna ou maligna, ele pode dizer que ela é maligna e pedir a remoção ou dizer que ela é benigna. Se na dúvida ele sempre disser que a pinta é maligna, ele estará aumentando a abrangência, mas diminuindo a precisão (pois haverá mais verdadeiros positivos e mais falso positivos). No limite, dizer que todas pintas são malignas dá abrangência máxima (100\%). Se ele sempre disser que a pinta duvidosa é benigna, ele aumentará a precisão, mas diminuirá a abrangência (pois haverá mais falso negativos e menos falso positivos).

\subsubsection{Fscore}
Como pode ser visto, a abrangência e a precisão possuem comportamentos antagônicos. Normalmente é importante que o classificador tenha tanto uma precisão aceitável, quanto uma abrangência razoável. Então combinou-se a abrangência e a precisão em um único número, chamado de Fscore.

O caso mais simples do Fscore (chamado de F1score) é uma média harmônica entre a precisão e a abrangência.

$F_1 = \frac{2}{\frac{1}{precisao} + \frac{1}{abrangencia}}$

Ou seja:

$F_1 = \frac{2.precisao.abrangencia}{precisao + abrangencia}$

Isto pode ser simplificado para:

$F_1 = \frac{2tp}{2tp+fn+fp}$

É interessante notar que se a precisão ou se a abrangência forem bem pequenas, o F1score será bem pequeno. Se ambas forem grandes, o F1score será algum valor entre as duas.

Como dependendo da aplicação, a precisão ou a abrangência podem ser mais importantes, há uma formulação genérica para o Fscore que utiliza o parâmetro $\beta$ para determinar qual das duas métricas é mais importante.

$F_\beta = \frac{(1+\beta^2).precisao.abrangencia}{\beta^2 precisao + abrangencia}$

É fácil provar que:

$F_\beta = \frac{(\beta^2+1)tp}{(\beta^2+1)tp + \beta^2fn + fp}$

Os dois valores de $\beta$ mais comuns são 0.5 e 2. Nota-se que para $\beta$ maior que 1 a abrangência é considerada mais importante. Para $\beta$ menor que 1 a precisão é considerada mais importante. 

\subsection{Classificador \emph{Multi-Class}}
As mesmas métricas definidas acima podem ser aplicadas para problemas \emph{multi-class}, entretanto alguns problemas surgem. Por exemplo, as métricas seriam calculadas individualmente para cada classe (gerando vários números), o que tornaria a avaliação do classificador um processo complicado (vários números são mais difíceis de serem analisados do que apenas um único).

A tabela \ref{tab:avaliacao_classificador_multiclass} resume as diferentes medidas utilizadas na avaliação de performance de um classificador \emph{multi-class}.



\subsubsection{Matriz de Confusão (\emph{Confusion Matrix})}
Para facilitar a análise de problemas \emph{multi-class}, criou-se a matriz de confusão. Ela consiste em uma matriz de números na qual as linhas representam as classificações realizadas pelo classificador e as colunas representam as classes originais das quais os objetos pertencem. A célula $a_{ij}$ da matriz possui um número que indica quantas vezes o classificador classificou objetos de tipo j como i.

É fácil de notar que uma matriz de confusão de um classificador perfeito possui vários números positivos na diagonal principal e zero em todas as outras células. Nota-se que qualquer numero maior que zero em uma célula que não pertence a diagonal principal representa um erro. Com a matriz de confusão fica fácil de visualizar os erros mais comuns feitos pelo classificador.

A Figura \ref{fig:confusion_matrix_example} mostra uma matriz de confusão de um classificador que deve classificar expressões faciais.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{confusion_matrix_example.png}
	\caption{Exemplo de Matriz de Confusão para um classificador retirado de \cite{bollano2007method}}
	\label{fig:confusion_matrix_example}
\end{figure}

A partir da matriz de confusão é fácil de determinar a precisão e a abrangência para cada classe i, conforme segue:

$precisao(i) = \frac{a_{ii}}{\sum_{j=1}^{L} a_{ij}} $

$abrangencia(i) = \frac{a_{ii}}{\sum_{j=1}^{L} a_{ji}} $

Ou seja, a precisão é um dos elementos da diagonal principal dividido pela soma de sua linha, enquanto que a abrangência é um dos elementos da diagonal principal dividido pela sua coluna.

Apesar de ter números específicos é muito bom pra analisar o quão bom um classificador é, é sempre importante observar a Matriz de Confusão pra detectar possíveis problemas e os lugares onda há erros mais comuns.

\subsubsection{Médias micro e macro (\emph{Micro and Macro Averaging})}
Conforme o descrito, pode-se calcular a precisão, a abrangência, o Fscore e a acurácia para cada classe individualmente. Entretanto seria interessante juntar todos estes dados em um único número. Para tal, faz-se uma média. Existem duas formas de realizar esta média, que são chamadas de micro e macro. 

A média micro consiste em somar os numeradores e denominadores das formulas das métricas supracitadas separadamente (na prática isto é equivalente a realizar uma média ponderada nos denominadores). Seguem as fórmulas de precisão, abrangência e acurácia micro.

$precisao_\mu = \frac{\sum_{i=1}^{L} {tp_i}}{\sum_{i=1}^{L} {tp_i + fp_i}}$

$abrangencia_\mu = \frac{\sum_{i=1}^{L} {tp_i}}{\sum_{i=1}^{L} {tp_i + fn_i}}$

Com uma matemática simples da pra concluir que a precisão micro é numericamente igual à abrangência micro (basta notar que o numerador dos dois é igual e o numerador dos dois representa a soma de todos os elementos da matriz de confusão, um através da soma das linhas e o outro através da soma das colunas).

A média macro consiste em somar os valores finais das métricas calculadas em cada classe e dividir pelo total de classes (uma média aritmética simples). Seguem as fórmulas de precisão e abrangência macro.

$precisao_M = \frac{\sum_{i=1}^{L} {\frac{tp_i}{tp_i + fp_i}}}{L}$

$abrangencia_M = \frac{\sum_{i=1}^{L} {\frac{tp_i}{tp_i + fn_i}}}{L}$

A intuição por trás dessas duas médias é que a média macro dá igual importância para todas as classes, enquanto que a média micro da mais importância para as classes mais comuns (no caso da abrangência) ou para as classes com mais objetos classificados nelas (no caso da precisão).

\subsubsection{Fscore}
Da mesma forma que foi definido o Fscore para o caso do classificador binário, é possível definir um Fscore macro e um Fscore micro (dependendo da utilização da precisão e abrangência macro ou micro).

$F_{\beta \mu} = \frac{(1+\beta^2).precisao_\mu .abrangencia_\mu}{\beta^2 precisao_\mu + abrangencia_\mu}$

$F_{\beta M} = \frac{(1+\beta^2).precisao_M .abrangencia_M}{\beta^2 precisao_M + abrangencia_M}$

Uma observação interessante é que, como a precisão micro e a abrangência micro são numericamente iguais, temos que o Fscore micro tambem terá este mesmo valor (independentemente de $\beta$). Para a média macro isto não acontece.

\subsubsection{Acurácia}

É possível encontrar na literatura duas definições diferentes de acurácia \cite{caballero2010sensitivity} \cite{sokolova2009systematic}. 

Em uma das definições, a acurácia de um classificador \emph{multi-class} é a média aritmética (média macro) das acurácias individuais de cada classe.

$accuracy = \frac{\sum_{i=1}^{L} {\frac{tp_i + tn_i}{tp_i + fn_i + fp_i + tn_i}}}{L}$

Na outra definição, a acurácia é a quantidade total de acertos (considerando todas as classes), dividida pela quantidade total de documentos.

\begin{equation}
accuracy = \frac{\sum_{i=1}^{L}{tp_i}}{N}
\label{eq:accuracy_multi_class}
\end{equation}

Considerando a matriz de confusão, temos que:

$accuracy = \frac{\sum_{i=1}^{L}{a_{ii}}}{\sum_{i=1}^{L}{\sum_{j=1}^{L}{a_{ij}}}} = \frac{\sum_{i=1}^{L}{a_{ii}}} {N}$

Neste trabalho será utilizada esta última definição por ser mais intuitiva e dar mais informação sobre a qualidade do classificador (observe que a primeira definição de acurácia tende a sempre resultar em valores altos para problemas com muitas classes, uma vez que a quantidade de verdadeiros negativos tende a ser alta para todas as classes).

Observa-se que a acurácia também é numericamente igual à abrangência e à precisão com média micro.

\subsubsection{Medida Kappa}
Como já foi discutido previamente, a acurácia é uma medida muito ruim pois é possível desenvolver classificadores com comportamento trivial que possuem uma boa acurácia. Para resolver este problema, utilizam-se outras estatísticas em vez da acurácia que possuem mais valor semântico e dão mais informações sobre a performance do classificador. Já foram explicadas outras medidas que possuem mais informações (abrangência, precisão e Fscore micros e macros) e a matriz de confusão que permite a visualização de problemas recorrentes no classificador. Entretanto, para o problema \emph{multi-class} é comum utilizar uma outra métrica (chamada de Kappa) para avaliar a performance do classificador em questão.

A estatística Kappa é utilizada para comparar a Acurácia Observada com a Acurácia Esperada. Esta estatística é muito poderosa e permite a realização de comparações até mesmo entre classificadores diferentes \cite{kappa_statistic}. Ela utiliza a comparação em relação a um classificador aleatório de modo a tornar esta estatística mais confiável que uma simples acurácia.

A Acurácia Observada é a mesma acurácia que já foi definida previamente na equação \ref{eq:accuracy_multi_class}. Ela representa a quantidade de objetos classificados corretamente em relação ao total.

Define-se como Acurácia Observada, a porcentagem esperada de acertos que um classificador aleatório teria se ele classificar os objetos com a mesma proporção entre as classes que o classificador original. 

Para se deduzir a fórmula da Acurácia Esperada primeiro calcula-se a quantidade de vezes que o classificador original escolheu a classe $c_i$. É fácil de ver que ele escolheu esta classe $tp_i + fp_i$ vezes. O valor esperado do número de acertos de um classificador aleatório que escolha a dada classe $tp_i + fp_i$ vezes é: 

$E[c_i] = (tp_i + fp_i)\frac{tp_i + fn_i}{N}$ 

Isto ocorre pois $\frac{tp_i + fn_i}{N}$ é a probabilidade de que um objeto aleatório pertença à classe $i$. A quantidade esperada para o número de acertos em todas as classes é de: 

$E[c_1,c_2,...,c_L] = \sum_{i=1}^{L} {(tp_i + fp_i)\frac{tp_i + fn_i}{N}}$

Como a Acurácia Esperada é a porcentagem de acertos desse classificador aleatório tem-se:

$acuracia\_esperada = \frac{E[c_1,c_2,...,c_L]}{N} =  \frac{\sum_{i=1}^{L}{(tp_i + fp_i)(tp_i + fn_i)}}{N^2}$

Em termos da matriz de confusão, a acurácia esperada é a somatória dos produtos entre cada linha com a sua respectiva coluna (linha 1 com coluna 1, linha 2 com coluna 2 e assim por diante) dividida pela quantidade total de elementos ao quadrado.

A definição de Acurácia Esperada é importante pois um classificador de Acurácia Observada 80\% é muito mais impressionante se a Acurácia Esperada for de 1\% do que se ela for 79\%.

A estatística $Kappa$ indica a porcentagem de melhoria que o classificador tem em relação à Acurácia Esperada \cite{viera2005understanding}. Um classificador com Acurácia Observada idêntica à Acurácia Esperada possui $Kappa=0$, enquanto que um classificador com Acurácia Observada de 100\% possui $Kappa=1$. Valores intermediários do $Kappa$ indicam a melhoria do classificador em relação ao aleatório. Desta forma, a fórmula para o $Kapp$ é:

$Kappa = \frac{acuracia\_observada - acuracia\_esperada}{1 - acuracia\_esperada}$


\section{Revisão Bibliográfica}
% TODO: Colocar aqui outros trabalhos de assuntos relacionados que ja existem na literatura e explicar porque o nosso trabalho é diferente (dizer que rede social adiciona uma complexidade a mais?)

