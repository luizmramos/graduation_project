\section{Definição formal}
Utilizando a notação adotada por Dan Jurafsky e Christopher Manning em seu curso de Processamento de Linguagem Natural para Stanford \cite{text_classification}, define-se o problema da classificação supervisionada de textos da seguinte forma.

Seja $C=\{c_1, c_2, ..., c_J\}$ um conjunto fixo de classes, $D=\{d_1, d_2, ..., d_n\}$ um conjunto de documentos, e $\mathcal{T}=\{(d_1, c_{d_1}), (d_2, c_{d_2}), ... , (d_m, c_{d_m})\}$ um conjunto de treinamento (subconjunto de $D$) com $m$ documentos classificados manualmente, o classificador consiste em uma função $\gamma :D\rightarrow C$ que relaciona um documento a uma classe, e um algoritmo de aprendizado de máquina supervisionado é um algoritmo que recebe como parâmetros $C$ e $\mathcal{T}$ e retorna $\gamma$.

\section{Tipos de classificadores}
Existem diversos tipos diferentes de classificadores que possuem resultados muito bons dependendo do problema analisado. Segue abaixo uma lista dos principais classificadores existentes.

\begin{itemize}	
	\item Árvores de Decisão
	\item Naïve Bayes
	\item Regressão Logística
	\item Support Vector Machines
	\item k-Nearest Neightbors
	\item Redes Neurais
	\item Dentre outros
\end{itemize}

A performance de todos estes métodos varia consideravelmente dependendo da aplicação. Estudos mostram que para bases de dados grandes o suficiente, ótimos resultados podem ser atingidos independentemente do método utilizado \cite{practical_issues}. Entretanto, para uma quantidade pequena de dados as Naïve Bayes apresentam resultados bons por serem classificadores de alto \emph{bias} / baixa variância \cite{quora_classifier}. 

As NB possuem as vantagens de serem fáceis de se implementar, serem bem rápidas na hora da execução e mostrarem bons resultados práticos.

\section{Redes Bayesianas}

\subsection{Definição}
Em modelagem gráfica probabilística, Redes Bayesianas são grafos direcionados que representam relações de dependência condicionais entre diferentes variáveis aleatórias \cite{introduction_to_graphical_models}. A partir da visualização de uma Rede Bayesiana é possível utilizar a regra de Bayes para realizar inferências e descobrir a probabilidade de eventos, dadas algumas variáveis observadas. As arestas direcionadas representam noções de dependência condicional entre as variáveis. Para cada nó do gráfico deve haver uma tabela de probabilidades condicionais para a variável em questão.

A Figura \ref{fig:bayesian_networks}, retirada do curso de Modelagem Gráfica Probabilística da professora Daphne Koller de Stanford \cite{probabilistic_graphical_models}, ilustra um exemplo de uma Rede Bayesiana simples. Neste caso, pode-se ver que a nota do aluno é influenciada pela dificuldade da prova e pela sua inteligencia. Alem disso a possibilidade do professor escrever uma carta de recomendação depende apenas da nota do aluno e o SAT do aluno depende apenas de sua inteligencia. Em cada nó do grafo há uma tabela de distribuições de probabilidades condicionais.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{bayesian_networks.png}
	\caption{Exemplo de Redes Bayesiana}
	\label{fig:bayesian_networks}
\end{figure}


\subsection{Regra de Bayes}
Para realizar inferências nas redes Bayesianas utiliza-se a regra de Bayes, como definida a seguir.

Sejam $A$ e $B$ dois eventos com probabilidades de ocorrência $P(A)$ e $P(B)$ e sendo $P(A|B)$ e $P(B|A)$ as probabilidades condicionais de $A$ dado $B$ e de $B$ dado $A$, respectivamente, tem-se que:

$$P(A|B) = \frac{{P(A) P(B|A)}}{P(B)}$$

Esse resultado parte da noção probabilidade conjunta $P(A,B)$.

$$P(A,B)=P(A) P(B|A) = P(B) P(A|B) \rightarrow P(A|B) = \frac{{P(A) P(B|A)}}{P(B)}$$

No caso do exemplo da Figura \ref{fig:bayesian_networks}, podemos calcular a probabilidade conjunta da rede da seguinte forma:

$$P(D,I,G,S,L)=P(D)P(I)P(G|I,D)P(S|I)P(L|G)$$

Onde $D=Difficulty$, $G=Grade$, $I=Intelligence$, $S=SAT$ e $L=Letter$.

\subsection{Naïve Bayes}
\subsubsection{Problema a ser resolvido}
Redes Bayesianas são ferramentas excelentes para modelar problemas complexos, entretanto elas possuem um grande problema prático. A realização de inferências em redes genéricas é um problema NP-Hard, conforme demonstrado por Cooper \cite{inference_bayesian_networks}.

O que é realizado na prática é, ou realizar inferências aproximadas nas redes com algoritmos polinomiais, ou simplificar as redes a alguns tipos específicos mais simples.

\subsubsection{O que são Naïve Bayes}
As Naïve Bayes (Bayes "Ingênuas") são simplificações feitas na modelagem de um problema por redes Bayesianas de modo a tornar possível realizar a inferência de forma rápida. Elas assumem que as variáveis do problema são condicionalmente independentes (mesmo que na prática elas não sejam, o que explica o nome \emph{ingênuas}).

A Figura \ref{fig:naive_bayes_example} mostra um exemplo de uma NB comum. Ela possui uma variável $Classe$ que interfere em uma série de outras varáveis $x_1, x_2, x_3, ... , x_n$ (que serão chamadas a partir daqui de features). É importante notar que a estrutura da rede mostra que as features são condicionalmente independentes umas das outras e a $Classe$ depende de cada uma das features individualmente. É fácil entender a grande vantagem desta abordagem. Cada tabela de probabilidades condicionais será pequena. Alem disso a inferência da variável classe, dadas algumas das features será bem simples, como será mostrado nas próximas seções.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{naive_bayes_example.jpg}
	\caption{Exemplo de uma Naïve Bayes}
	\label{fig:naive_bayes_example}
\end{figure}

Na prática o NB é um ótimo classificador de textos, pois, além de ser simples, traz resultados comparáveis a outros classificadores substancialmente mais complexos e lentos.

\subsubsection{Independência Condicional}
Antes de explicar a inferência em NB é importante entender o que significa o conceito de independência condicional.

Dados dois eventos $A$ e $B$, dizemos que eles são independentes se a probabilidade de ocorrência de um deles não é influenciada pelo fato do outro ter ocorrido. Ou seja:

$$P(A|B) = P(A)$$
$$P(B|A) = P(B)$$

Esta propriedade é muito relevante na simplificação de expressões pois podemos usar o fato de que a probabilidade conjunta é igual ao produto das probabilidades individuais.

$$P(A,B) = P(A)P(B|A) = P(A)P(B)$$

\subsubsection{Inferência}
A regra de Bayes seguida pela propriedade de independência condicional pode ser utilizada para a realização de inferência em NB da seguinte forma.

Sejam $C=\{c_1,c_2,c_3, ..., c_J\}$ um conjunto de classes e $x_1, x_2, x_3, x_4 ..., x_n$ as features a serem analisadas. Como trata-se de NB, assume-se independência condicional das features. Deseja-se saber para cada $i$:

$$P(C=c_i|x_1,x_2,...,x_n)$$

Ou seja, deseja-se saber qual a probabilidade da classe possuir o valor $c_i$, dadas as features observadas.

Pela regra de Bayes temos:

\begin{equation}
P(C=c_i|x_1,x_2,...,x_n) = \frac{P(x_1,x_2,...,x_n|C=c_i)P(C=c_i)}{P(x_1,x_2,...,x_n)}
\label{eq:bayes_inference}
\end{equation}

Como $x_1, x_2, x_3, ..., x_n$ são condicionalmente independentes entre si, temos:


\begin{equation}
\begin{split}
P(x_1,x_2,...,x_n|C=c_i) &= P(x_1,x_2,...,x_{n-1}|x_n,C=c_i)P(x_n|C=c_i) \\ 
&= P(x_1,x_2,...,x_{n-1}|C=c_i)P(x_n|C=c_i) \\
&= P(x_1,x_2,...,x_{n-2}|x_{n-1},C=c_i)P(x_{n-1}|C=c_i)P(x_n|C=c_i) \\
&= P(x_1,x_2,...,x_{n-2}|C=c_i)P(x_{n-1}|C=c_i)P(x_n|C=c_i) \\
&= ... \\  
&= P(x_1|C=c_i)P(x_2|C=c_i)P(x_3|C=c_i)...P(x_n|C=c_i)  \\
&= \prod_{j=1}^{n}P(x_j|C=c_i)
\end{split}
\label{eq:produtorio}
\end{equation}


Substituindo \ref{eq:produtorio} em \ref{eq:bayes_inference} temos:

\begin{equation}
P(C=c_i|x_1,x_2,...,x_n) = \frac{P(C=c_i){\prod_{j=1}^{n}P(x_j|C=c_i)}}{P(x_1,x_2,...,x_n)}
\label{eq:naive_bayes_probability}
\end{equation}

\subsubsection{Classificação utilizando Naïve Bayes}
No problema de classificação temos um documento $d$ que será representado pelas features $x_1,x_2,...,x_n$ e deseja-se saber a qual classe $c_i \in C$ este documento pertence. Matematicamente, deseja-se saber à qual classe é mais provável que o documento pertença, isto é, a classe que maximiza $P(C=c_i|d)$.

\begin{align*}
\gamma(d) &= argmax_{c_i \in C}(P(C=c_i|d)) \\
&= argmax_{c_i \in C}(P(C = c_i|x_1,x_2,...,x_n))
\end{align*}


Pela equação \ref{eq:bayes_inference} e eliminando-se os termos comuns a todas as classes, tem-se que:

\begin{equation}
\gamma(d) = argmax_{c_i \in C}(P(x_1,x_2,...,x_n|C=c_i)P(C=c_i))
\label{eq:nb_doc}
\end{equation}

Aplicando-se \ref{eq:naive_bayes_probability} e eliminando-se os termos comuns novamente, tem-se que:

\begin{equation}
\gamma(d) = argmax_{c_i \in C}\left(P(C=c_i)\prod_{j=1}^{n}P(x_j|C=c_i)\right)
\label{eq:nb_doc_indep}
\end{equation}

\subsection{Modelagens para classificação de texto}

Uma vez que já foi entendida a forma de classificar o texto, resta representá-lo por um conjunto de features adequada e adotar uma distribuição (parametrizada ou não) para suas probabilidades. A seguir é explicado o modelo de representação de documentos \emph{Bag-of-words} e as modelagens \emph{Naïve Bayes de Bernoulli} e \emph{Multinomial Naïve Bayes}, onde a principal diferença é a distribuição das probabilidades.

\subsubsection{Representação por Bag-of-words}

Em classificação de texto uma das tarefas especialmente importante é a de representação de documento e seleção das features a serem consideradas. Diz-se especialmente importante pois, diferentemente de outras tarefas de classificação, a classificação textual apresenta dimensionalidade bastante elevada e a presença de muitas features irrelevantes (\emph{noise}).
	
O modelo \emph{Bag-of-words} é uma representação simplificada de um texto ou documento em que o mesmo é figurativamente imaginado como uma sacola onde as palavras são colocadas, ignorando gramática ou ordem das palavras, mas mantendo a multiplicidade de cada uma.

Muito utilizado em classificação de texto (presente caso), \emph{Bag-of-words} pode ser entendido como um mapa entre as palavras do texto e o número de ocorrências de cada uma.

Em alguns casos pode-se calcular a representação \emph{Bag-of-words} de um documento levando-se em consideração todas as palavras do mesmo ou somente um subconjunto das mesmas. Nas figuras \ref{fig:bag_of_words_1}, \ref{fig:bag_of_words_2}, \ref{fig:bag_of_words_3}, \ref{fig:bag_of_words_4} apresenta-se em sequência o método prático de reduzir um documento na sua representação simplificada \emph{Bag-of-words} levando-se em consideração um subconjunto das palavras do mesmo.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{bag_of_words_1.png}
	\caption{Exemplo de documento em sua fiel representação sendo classificado. Fonte: \cite{text_classification}}
	\label{fig:bag_of_words_1}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{bag_of_words_2.png}
	\caption{Exemplo de documento com as palavras relevantes destacadas. Fonte: \cite{text_classification}}
	\label{fig:bag_of_words_2}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{bag_of_words_3.png}
	\caption{Exemplo de documento somente com as palavras relevantes. Fonte: \cite{text_classification}}
	\label{fig:bag_of_words_3}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{bag_of_words_4.png}
	\caption{Exemplo de documento com as palavras relevantes e seu número de ocorrências, onde a ordem não conta. Fonte: \cite{text_classification}}
	\label{fig:bag_of_words_4}
\end{figure}

Como pode-se perceber ao final da \ref{fig:bag_of_words_4} gerou-se um mapa de palavra para número de ocorrência das mesmas, onde a ordem não é considerada. Apesar de ignorar a ordem das palavras (e dependendo do pré-processamento outras informações como pontuação) é importante manter em mente que um humano facilmente classificaria artigos com taxa relativamente alta de acerto se fossem fornecidas algumas palavras-chave do texto fora de contexto e ordem. Pela sua simplicidade e eficiência para uso em classificação de texto, foi decidido utilizar a representação de documentos por Bag-of-words.

\subsubsection{Naïve Bayes de Bernoulli}

Na modelagem de Bernoulli, o documento é representado por um vetor de features onde cada coordenada indica se determinada palavra está presente ou não no documento, isto é, o documento é um vetor $\boldsymbol{x}$ de tamanho $|V|$, onde $V$ é o vocabulário, e $x_i \in \{0, 1\}$ indica se a palavra $w_i$ está ou não presente no documento (caso apareça mais de uma vez $x_i$ permanece sendo $1$). Assim:

\begin{align}
P(\boldsymbol{X} = \boldsymbol{x} | c) &= \prod_{i=1}^{|V|}P(U_i=1|c)^{x_i}P(U_i=0|c)^{(1-x_i)}\notag\\
&= \prod_{i=1}^{|V|}P(U_i=1|c)^{x_i}(1-P(U_i=1|c))^{(1-x_i)}\notag\\
&= \prod_{i=1}^{|V|}p_i^{x_i}(1-p_i)^{(1-x_i)}
\end{align}

$$p_i = P(U_i|c)$$

Onde $U_i$ é uma variável aleatória que indica a se a palavra $w_i$ está ou não presente num documento de classe $c$.
$p_i$ é estimado como a fração de documentos com class $c$ que possui a palavra $w_i$, isto é:

$$p_i = P(U_i = 1|c) \approx \frac{\#(\text{documentos de classe } c \text{ que contém } 1 \text{ ou mais ocorrências de } w_i) }{\#(\text{documentos de classe } c)} $$

\subsubsection{Multinomial Naïve Bayes}
 
Nessa modelagem, o número de vezes em que uma determinada palavra aparece em um documento é levada em consideração. É bastante comum sua utilização em classificação de texto. 

No Multinomial NB, também representa-se o texto por uma \emph{Bag-of-words}. Utiliza-se a distribuição multinomial para modelar as probabilidades das features. Para derivar a expressão que representa $P(x_1,x_2,...,x_n|c)$ primeiro define-se o modelo de cada documento e o que representa seu conjunto de features.

Suponha que o documento $d$ seja representado por $\boldsymbol{d}$, um vetor, que gere o vetor de features	$\boldsymbol{x}(\boldsymbol{d}) = (x_1, x_2, ..., x_{|V|})$, onde $x_i$ representa a quantidade de vezes em que a palavra $w_i$ ocorreu no documento $\boldsymbol{d}$. Note que dessa forma dois documentos cuja ordem das palavras mude possui o mesmo vetor de features, como esperado pela hipótese \emph{Bag-of-words}. 

Um documento $d$ pode ser representado por um vetor $\boldsymbol{d}$ de tamanho $|\boldsymbol{d}|$, o número de palavras do documento, onde cada coordenada é o índice da palavra do vocabulário que está naquela posição. Isto é, $\boldsymbol{d} = (d_1, d_2, ..., d_{|\boldsymbol{d}|})$, onde $d_j \in \{1, 2, ..., |V|\}$, o que significa que a palavra na $j$-ésima posição é $w_{d_j}$. Daí:

$$x_i = |\{d_j | j \in \{1, 2, ..., |d|\} \land d_j = i\}|$$

Abusando-se da notação utilizar-se-á $\boldsymbol{d}$ para representar o documento $d$, já que o mesmo torna-se unicamente definido.

Por exemplo, um documento "$w_6 w_4 w_5 w_9 w_6 $" é representado por $\boldsymbol{d} = (6, 4, 5, 9, 6)$ onde $|\boldsymbol{d}| = 5$. Supondo $|V| = 10$, esse documento gera um vetor de features $\boldsymbol{x}(\boldsymbol{d}) = (0, 0, 0, 1, 1, 2, 0, 0, 1, 0)$, observe que existem $\frac{5!}{2!}$ documentos que geram o mesmo vetor de features, o que nada mais é que o número de permutações de $\boldsymbol{d}$. 

De maneira genérica, para um vetor de features $\boldsymbol{x^{(j)}}$, seja $docs(\boldsymbol{x}^{(j)})$ o conjunto de documentos que o geram:

$$docs(\boldsymbol{x}^{(j)}) = \{\boldsymbol{d} | \boldsymbol{x}(\boldsymbol{d}) = \boldsymbol{x}^{(j)} = (x^{(j)}_1, x^{(j)}_2, ..., x^{(j)}_{|V|})\}$$

Tem-se que:

$$|\boldsymbol{d}| = \sum_{i = 1}^{|V|}x_i^{(j)} (\forall\boldsymbol{d} \in docs(\boldsymbol{x}^{(j)}))$$

E:

\begin{align*}
|docs(\boldsymbol{x}^{(j)})| &= \text{número de permutações de um } \boldsymbol{d} \in docs(\boldsymbol{x}^{(j)}) \\
&= \frac{|\boldsymbol{d}|!}{\prod_{i=1}^{|V|}x_i^{(j)}!} = \frac{(\sum_{i=1}^{|V|}x_i^{(j)})!}{\prod_{i=1}^{|V|}x_i^{(j)}!}
\end{align*}

Seja $D_i$ a variável aleatória que representa a $i$-ésima palavra de um documento $\boldsymbol{d}$. Assume-se que:

\begin{equation}
P(D_i = k | c) = P(D_j = k | c) = P(D = k | c)
\label{eq:multi_positional_indep}
\end{equation}
$$\forall i, j \in \{1, 2, ..., |\boldsymbol{d}|\}$$
$$\forall k \in \{1, 2, ..., |V|\}$$

Em outras palavras, a distribuição de probabilidade de quais palavras aparecem no documento dado sua classe independe da posição da palavra. A essa hipótese dá-se o nome de \emph{Independência Posicional}. Assim, podemos eliminar o subscrito e escrever $D$ para representar essa distribuição.

Pela hipótese de \emph{Independência Condicional} de NB aplicada às distribuições $D$, tem-se também que:
\begin{equation}
P(D_i|D_j,c) = P(D_i|c)
\label{eq:multi_conditional_indep}
\end{equation}
Logo, seja $\boldsymbol{d} = (d_1, d_2, ..., d_N)$ um documento, por \ref{eq:multi_conditional_indep} e \ref{eq:multi_positional_indep}, vem:
$$P(\boldsymbol{D} = \boldsymbol{d} | c) = \prod_{j=1}^{N}P(D_j = d_j | c) = \prod_{j=1}^{N}P(D = d_j | c)$$
Levando-se em consideração $\boldsymbol{x}(\boldsymbol{d})$, tem-se:
$$\prod_{j=1}^{N}P(D=d_j|c)=\prod_{i=1}^{|V|}P(D=i|c)^{x_i}$$
Isto é, condensa-se probabilidades de palavras iguais com expoente $>1$, adiciona-se termos elevados a $0$ para as palavras que não estão no documento e reordena-se na ordem em que as palavras aparecem no vocabulário. Assim:
$$P(\boldsymbol{D} = \boldsymbol{d} | c) = \prod_{i=1}^{|V|}P(D = i | c)^{x_i}$$
Note que $P(\boldsymbol{D} = \boldsymbol{d} | c)$ só depende de $\boldsymbol{x}(\boldsymbol{d})$, que é igual para todo documento $\in docs(\boldsymbol{x}(\boldsymbol{d}))$. 
Finalmente:
\begin{align}
P(\boldsymbol{X} = \boldsymbol{x} | c) &= \sum_{d \in docs(\boldsymbol{x})} P(\boldsymbol{D} = \boldsymbol{d} | c)\notag\\
&= \sum_{d \in docs(\boldsymbol{x})} \prod_{i=1}^{|V|}P(D=i|c)^{x_i}\notag\\
&= |docs(\boldsymbol{x})| \prod_{i=1}^{|V|}P(D=i|c)^{x_i}\notag\\
&= \frac{(\sum_{i=1}^{|V|}x_i)!}{\prod_{i=1}^{|V|}x_i!} \prod_{i=1}^{|V|}P(D=i|c)^{x_i}
\label{eq:multinomial_nb}
\end{align}

Pode-se chegar a essa equação através da hipótese da distribuição multinomial. Nessa distribuição existem $n$ tentativas independentes, cada uma gerando uma de $k$ categorias distintas, cada categoria com uma chance distinta de sucesso. Considera-se cada tentativa a ocorrência de determinada palavra em uma posição do documento (que contém $n$ palavras) dada sua classe $c$ e cada categoria como sendo uma palavra do vocabulário, de tal forma que $k = |V|$. 

Os parâmetros da distribuição são as probabilidades de cada categoria, ou palavra, $\boldsymbol{p} = (p_1, p_2, ..., p_{|V|})$ de tal forma que $\sum_{i=1}^{|V|} p_i = 1$. Tem-se, então, que $p_j = P(D = j | c)\ (\forall j \in {1, 2, ..., |V|})$. Diz-se então que o vetor $\boldsymbol{X} = (X_1, X_2, ..., X_{|V|})$ segue uma distribuição multinomial com $\sum_{i=1}^{|V|}X_i = n$ e que:

\begin{align*}
P(\boldsymbol{X} = \boldsymbol{x} | c) &= \frac{n!}{x_1!x_2!...x_3!}p_1^{x_1}p_2^{x_2}...p_{|V|}^{x_{|V|}}\\
&= \frac{(\sum_{i=1}^{|V|}x_i)!}{\prod_{i=1}^{|V|}x_i!}\prod_{i=1}^{|V|}p_i^{x_i}\\ 
&= \frac{(\sum_{i=1}^{|V|}x_i)!}{\prod_{i=1}^{|V|}x_i!}\prod_{i=1}^{|V|}P(D=i|c)^{x_i}
\end{align*}

Voltando à função $\gamma$ para determinação da classe de um documento, sabemos que:

$$P(x_1, x_2, ..., x_n | C = c_i) = P(\boldsymbol{X}=\boldsymbol{x}|c_i)$$

Então, com \ref{eq:nb_doc}, vem:

$$\gamma(d) = argmax_{c_i \in C}(P(\boldsymbol{X}=\boldsymbol{x}|c_i)P(C=c_i))$$

Utilizando-se \ref{eq:multinomial_nb} e eliminando os termos comuns a todas as classes.

$$\gamma(x_1, x_2, ..., x_N) = argmax_{c_i \in C}\left(P(C=c_i)\prod_{j=1}^{|V|}P(D=j|C = c_i)^{x_j}\right)$$

Agora, resta-se estimar $P(C=c_i)$, a probabilidade do documento ser de classe $c_i$, e $P(D=j|C=c_i)$, a probabilidade de encontrar $w_j$ num documento de classe $c_i$. Com um conjunto de treinamento suficientemente grande a estimativa fica bem próxima aos valores reais para o domínio. Logo, temos:

$$P(C=c_i) \approx \frac{\#(\text{documentos de classe } c_i)}{\#(\text{todos os documentos})}$$

$$P(D=j|C=c_i) \approx \frac{\#(\text{palavras } w_j \text{ em documentos de classe } c_i)}{\#(\text{total de palavras em documentos de classe } c_i)}$$

Note que apesar de ter-se utilizado a nomenclatura "palavra", qualquer feature que representasse outra entidade poderia ser utilizada. 

Finalmente, o problema de classificação torna-se um problema de contagem.

\subsubsection{Outras features}
É possível enriquecer o classificador utilizando outras features (que não sejam as próprias palavras do texto). Exemplos de features que podem ser utilizadas são: combinações de palavras, o tamanho do texto, quantidade de sinais de pontuação, quantidade de palavras com iniciais maiúsculas, etc.

Para o caso específico de postagens em redes sociais existem ainda outras features que podem ser incluídas. Pode-se considerar o autor da postagem, o momento em que ela foi publicada, a presença de fotos, vídeos ou links, etc.

\subsubsection{Smoothing}

Um dos problemas práticos encontrados por NB é a ocorrência de contagens nulas. O grande problema do 0 é que se ele for apenas um dos fatores da multiplicação o resultado inteiro será 0, inutilizando o método.

Isto não é algo incomum, principalmente se o conjunto de treinamento for pequeno. Basta ter uma palavra no conjunto de teste que nunca ocorreu em uma determinada classe no conjunto de treinamento.

Existem técnicas que são utilizadas para resolver este problema e elas são chamadas de Smoothing. Neste trabalho utilizou-se \emph{Laplace Smoothing}, uma das mais comuns.

A técnica consiste em assumir que todas as features foram vistas pelo menos $\alpha$ vezes em cada uma das classes. Isso se traduz nas seguintes formulas, sendo $L$ o número total de classes, $V$ o numero total de features, $N$ o número total de documentos e $n(c)$ o número de documentos de classe $c$.

$$P(C=c_i) \approx \frac{\#(\text{documentos de classe } c_i)  + \alpha}{\#(\text{todos os documentos}) + \alpha L}$$

$$P(D=j|C=c_i) \approx \frac{\#(\text{palavras } w_j \text{ em documentos de classe } c_i) + \alpha}{\#(\text{total de palavras em documentos de classe } c_i) + \alpha V}$$

No caso especial em que $\alpha = 1$, tem-se o \emph{Add-One Smoothing}.

\section{Weighted Naïve Bayes}
\label{sec:weighted_naive_bayes}
Um dos problemas das NB é que muitas vezes nas aplicações reais não é possível assumir a independência condicional das features. Um modo inicial de relaxar essa hipótese de independência, é eliminar features com alta correlação, fazendo com que o subconjunto restante se encaixe melhor na hipótese de independência condicional. Isto é chamado de seleção de features.

Neste caso temos:

$$\gamma(d) = argmax_c\left(P(C=c_i){\prod_{j=1}^{n}P(x_j|C=c_i)^{I(j)}}\right)$$

Onde:

$$I(j) \in \{0,1\}$$

Existem bastantes features que são dependentes de outras, dessa forma quando coloca-se ambas no produtorio acaba-se prejudicando o resultado. Logo, colocar peso nas features é uma forma de amenizar a forte hipótese de independência das NB. Uma abordagem mais genérica, então, é ponderar cada feature de acordo com sua relevância. Ou seja:

$$\gamma(d) = argmax_c\left(P(C=c_i){\prod_{j=1}^{n}P(x_j|C=c_i)^{w(j)}}\right)$$

Onde:

$$w(j) \in \mathbb{R}^+$$

Nota-se que a seleção de features é um caso específico da ponderação de features (onde $w(j)$ só pode ser 0 ou 1). Existem diversos algoritmos já desenvolvidos na literatura para seleção de features, muitos dos quais envolvem alguma busca no espaço das features com testes num grupo de dados dedicado e expansão do grupo de features somente se a nova feature gerar uma melhoria na classificação nesse grupo de dados. Esses algoritmos são brevemente explorados em \cite{zaidi2013alleviating}.

Tendo decidido realizar feature weighting, o grande problema passa a ser determinar os pesos $w(j)$ das features. Há diversos algoritmos que ja foram propostos para realizar esta tarefa. Intuitivamente, quer-se um peso maior para features que possam ser decisivas no resultado final (ajudam a distinguir determinada classe).

Neste trabalho, estudou-se a utilização de um método baseado na Teoria da Informação, que além de ser comum na literatura (\cite{weighted_naive_bayes} e \cite{zaidi2013alleviating}), possui fundamentos teóricos bem embasados, onde o \emph{Gain Ratio} de um atributo é utilizado como seu peso.
 
 \subsection{Weighted Naïve Bayes}

Em Teoria da Informação, entropia é o valor esperado de informação contido em uma mensagem. Praticamente falando, a distribuição de probabilidade de eventos, junto com a quantidade de informação de cada evento, compõe uma variável aleatória cujo valor esperado é a quantidade média de informação, ou entropia, gerada por essa distribuição. Assim entropia $H$ é:

\begin{equation*}
H(X) = E[-ln(P(X))]
\end{equation*}

Onde $-ln(P(x))$ é considerado o conteúdo de informação de $X$. Para distribuições discretas:

\begin{equation}
H(X) = -\sum_{i} P(x_i) log_b P(x_i)
\label{eq:entropy}
\end{equation}

Onde, geralmente, $b = 2$, de forma que a unidade de $H(X)$ é bits.

Entropia pode ser entendida como uma medida da incerteza de determinada variável. Considere uma moeda sendo jogada para o alto e uma probabilidade $p$ de o resultado dar cara. Quanto mais próximo de $p = 0,5$ maior é a incerteza do resultado, logo maior será a entropia, e quanto mais longe de $p = 0,5$ menor é a entropia. A figura \ref{fig:entropy_coin_toss} ilustra isso.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.4\textwidth]{entropy_coin_toss.png}
	\caption{Entropia vs $p$, probabilidade de dar cara, num lançamento de moeda}
	\label{fig:entropy_coin_toss}
\end{figure}

Semelhante à entropia, define-se o conceito de Informação Mútua entre duas variáveis aleatórias, que mede sua dependência. Formalmente, a informação mútua entre duas variáveis discretas $X$ e $Y$ é:

\begin{equation}
MI(X, Y)=\sum_{y \in Y}\sum_{x \in X}P(X=x, Y=y)log\left(\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}\right)
\label{eq:mutual_information}
\end{equation}

Intuitivamente, $MI(X, Y)$ mede o quanto de informação $X$ e $Y$ compartilham. Se, por exemplo, $X$ e $Y$ forem independentes tem-se $MI(X, Y) = 0$.

Em luz de \ref{eq:entropy} e \ref{eq:mutual_information} define-se Razão de Ganho \emph{GR} (do inglês \emph{Gain Ratio}) de uma feature $X_i$ como sendo:

\begin{equation}
GR(i) = \frac{MI(X_i, C)}{H(X_i)} = \frac{\sum_{c \in C}\sum_{x_1 \in X_1}P(x_1, c)log\left(\frac{P(x_1, c)}{P(x_1)P(c)}\right)}{-\sum_{x_1}P(x_1)log(P(x_1))}
\end{equation}

Observe que quanto mais dependentes forem $X_i$ de $C$, isto é, quanto mais os valores de $X_i$ contribuírem para classificar o documento como pertencendo a determinada classe, mais alto será $MI(X_i, C)$, justificando um aumento em $GR(i)$. 

Agora, resta-se somente normalizar as razões de ganho, de forma que o peso de cada feature é a sua razão de ganho dividia pela média de todas as razões de ganho.

\begin{equation}
w(j) = \frac{GR(j)}{\frac{1}{N}\sum_{i=1}^{N}GR(i)}
\end{equation}

Onde $N$ é o número de features.

\section{Métodos de avaliação de um classificador}

Uma vez desenvolvido o classificador é importante ser capaz de avaliá-lo a fim de determinar o quão bom ele é. A utilização de métricas numéricas para avaliar os classificadores é interessante pois permite a realização de comparações entre as diferentes versões implementadas, tornando possível determinar se as modificações que foram feitas estão fazendo efeito.

Existem diversas métricas que podem ser utilizadas para avaliar classificadores, algumas delas serão analisadas nesta seção \cite{sokolova2009systematic}.


\subsection{Classificador Binário}

Primeiro serão explorados os métodos de avaliação da performance do classificador binário, que é o classificador mais simples. 

A tabela \ref{tab:avaliacao_classificador_binario} resume as diferentes medidas que podem ser utilizadas, onde $tp$ é o verdadeiro positivo, $tn$ é o verdadeiro negativo, $fp$ é o falso positivo e $fn$ é o falso negativo.

\newcommand{\specialcell}[2][c]{%
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\begin{table}[tph]
	\begin{center}
		\begin{tabular}{c c c}
			\hline
			Medida & Fórmula & Intuição \\
			\hline 
			&&\\
			Acurácia & $\frac{tp+tn}{tp+fn+fp+tn}$ & Eficácia global do classificador \\[0.6cm]
			Precisão & $\frac{tp}{tp+fp}$ & \specialcell{Quantos dos objetos classificados como positivo \\são efetivamente positivos}\\[0.6cm]
			Abrangência & $\frac{tp}{tp+fn}$ & \specialcell{Quantos dos objetos positivos \\ foram classificados como positivos}\\[0.6cm]
			Fscore & $\frac{(\beta^2+1)tp}{(\beta^2+1)tp + \beta^2fn + fp}$ & \specialcell{Uma média harmônica entre \\ a precisão e a abrangência} \\[0.2cm]
			\hline
		\end{tabular}
	\end{center}
	\caption{Resumo das medidas de performance para classificadores binários}
	\label{tab:avaliacao_classificador_binario}
\end{table}

\subsubsection{Acurácia}
\label{sec:acuracia}
A acurácia mede a quantidade total de acertos (verdadeiro positivos ou verdadeiro negativos) em relação à quantidade total de objetos classificados.

$acuracia = \frac{tp+tn}{tp+fn+fp+tn}$

Esta é a medida de performance mais fácil de se entender, entretanto ela não é muito informativa, uma vez que dependendo da distribuição das classes, classificadores ruins podem ter alta acurácia.

Suponha, por exemplo, um classificador que classifica tumores em benignos ou malignos. Suponha também que 99.9\% dos tumores sejam benignos. É possível criar um classificador que sempre responde que o tumor é benigno e ele teria uma acurácia de 99.9\% (aparentemente muito boa). Entretanto este classificador falha miseravelmente na sua tarefa principal (que é determinar quais tumores são malignos para que as devidas providencias sejam tomadas).

\subsubsection{Precisão}
A precisão é uma estatística que indica quantos dos objetos classificados como positivo são efetivamente positivos. Quando um classificador com alto precisão classifica um objeto como positivo, é muito provável que ele seja positivo (entretanto nada se sabe sobre quantos objetos positivos ele esta classificando como negativo).

$precisao = \frac{tp}{tp+fp}$

Um classificador que indica para uma pessoa se ela deve investir ou não em uma determinada ação deve ser muito preciso. Pois para o investidor o importante é que quando ele siga o conselho do classificador e invista, ele não perca dinheiro. Não importa tanto se havia várias outras ações que eram boas, mas que o classificador desprezou.

\subsubsection{Abrangência (\emph{Recall})}
A abrangência de um classificador indica quantos dos objetos positivos são efetivamente classificados como positivos. Um classificador com grande abrangência provavelmente irá detectar a maior parte dos objetos positivos (apesar de também poder detectar alguns negativos como sendo positivos).

$abrangencia = \frac{tp}{tp+fn}$

Um médico que classifica pintas na pele das pessoas entre malignas ou benignas (câncer de pele ou não) deve ter uma grande abrangência (sendo a classificação 'maligno' a classe positiva do classificador), mas a precisão não é tão importante. Isso ocorre pois se o médico deixar de classificar alguma pinta que era maligna como maligna, o paciente pode acabar evoluindo a doença e morrer (ou seja, todos que são efetivamente malignos devem ser classificados como malignos). A precisão não é tão importante pois se por ventura o médico classificar uma pinta benigna como maligna, o paciente vai remover a mesma e na biópsia será possível identificar que não era nada demais (o paciente só terá que passar inutilmente pelo procedimento cirúrgico de remoção de pintas, mas isso não é muito problemático).

Existe, normalmente uma relação inversa entre a precisão e a abrangência. Em geral quanto se aumenta a precisão se reduz a abrangência e vice-versa. Pode-se exemplificar isto com o mesmo caso do médico que classifica pintas. Quando ele fica com dúvida se a pinta é benigna ou maligna, ele pode dizer que ela é maligna e pedir a remoção ou dizer que ela é benigna. Se na dúvida ele sempre disser que a pinta é maligna, ele estará aumentando a abrangência, mas diminuindo a precisão (pois haverá mais verdadeiros positivos e mais falso positivos). No limite, dizer que todas pintas são malignas dá abrangência máxima (100\%). Se ele sempre disser que a pinta duvidosa é benigna, ele aumentará a precisão, mas diminuirá a abrangência (pois haverá mais falso negativos e menos falso positivos).

\subsubsection{Fscore}
Como pode ser visto, a abrangência e a precisão possuem comportamentos antagônicos. Normalmente é importante que o classificador tenha tanto uma precisão aceitável, quanto uma abrangência razoável. Então combinou-se a abrangência e a precisão em um único número, chamado de Fscore.

O caso mais simples do Fscore (chamado de F1score) é uma média harmônica entre a precisão e a abrangência.

$F_1 = \frac{2}{\frac{1}{precisao} + \frac{1}{abrangencia}}$

Ou seja:

$F_1 = \frac{2.precisao.abrangencia}{precisao + abrangencia}$

Isto pode ser simplificado para:

$F_1 = \frac{2tp}{2tp+fn+fp}$

É interessante notar que se a precisão ou se a abrangência forem bem pequenas, o F1score será bem pequeno. Se ambas forem grandes, o F1score será algum valor entre as duas.

Como dependendo da aplicação, a precisão ou a abrangência podem ser mais importantes, há uma formulação genérica para o Fscore que utiliza o parâmetro $\beta$ para determinar qual das duas métricas é mais importante.

$F_\beta = \frac{(1+\beta^2).precisao.abrangencia}{\beta^2 precisao + abrangencia}$

É fácil provar que:

$F_\beta = \frac{(\beta^2+1)tp}{(\beta^2+1)tp + \beta^2fn + fp}$

Os dois valores de $\beta$ mais comuns são 0.5 e 2. Nota-se que para $\beta$ maior que 1 a abrangência é considerada mais importante. Para $\beta$ menor que 1 a precisão é considerada mais importante. 

\subsection{Classificador \emph{Multi-Class}}
As mesmas métricas definidas acima podem ser aplicadas para problemas \emph{multi-class}, entretanto alguns problemas surgem. Por exemplo, as métricas seriam calculadas individualmente para cada classe (gerando vários números), o que tornaria a avaliação do classificador um processo complicado (vários números são mais difíceis de serem analisados do que apenas um único).

A tabela \ref{tab:avaliacao_classificador_multiclass} resume as diferentes medidas utilizadas na avaliação de performance de um classificador \emph{multi-class}.

\begin{table}[tph]
	\begin{center}
		\begin{tabular}{c c c}
			\hline
			Medida & Fórmula & Intuição \\
			\hline 
			&&\\
			\specialcell{Precisão \\ Micro} & $\frac{\sum_{i=1}^{L} {tp_i}}{\sum_{i=1}^{L} {tp_i + fp_i}}$ & \specialcell{Média das precisões ponderando cada classe \\ de acordo com a quantidade de vezes que \\ ela é escolhida pelo classificador}\\[0.6cm]
			\specialcell{Precisão \\ Macro} & $\frac{\sum_{i=1}^{L} {\frac{tp_i}{tp_i + fp_i}}}{L}$ & \specialcell{Média das precisões dando \\ pesos iguais a cada classe}\\[0.6cm]
			\specialcell{Abrangência \\ Micro} & $\frac{\sum_{i=1}^{L} {tp_i}}{\sum_{i=1}^{L} {tp_i + fn_i}}$ & \specialcell{Média das abrangências ponderando cada classe \\ de acordo com a quantidade de vezes que \\ ela aparece no conjunto de testes}\\[0.6cm]
			\specialcell{Abrangência \\ Macro} & $\frac{\sum_{i=1}^{L} {\frac{tp_i}{tp_i + fn_i}}}{L}$ & \specialcell{Média das abrangências dando \\ pesos iguais a cada classe}\\[0.6cm]
			\specialcell{Acurácia \\ Observada} & $\frac{\sum_{i=1}^{L}{tp_i}}{N}$ & Eficácia global do classificador \\[0.6cm]
			\specialcell{Acurácia \\ Esperada} & $\frac{\sum_{i=1}^{L}{(tp_i + fp_i)(tp_i + fn_i)}}{N^2}$ & \specialcell{Eficácia esperada de classificador aleatório \\ que chute na mesma proporção \\ do classificador em analisado} \\[0.6cm]	
			Kappa & $\frac{acc\_obs - acc\_esp}{1 - acc\_esp}$ & \specialcell{Quantos dos objetos positivos \\ foram classificados como positivos}\\[0.6cm]
			\hline
		\end{tabular}
	\end{center}
	\caption{Resumo das medidas de performance para classificadores multi class}
	\label{tab:avaliacao_classificador_multiclass}
\end{table}


\subsubsection{Matriz de Confusão (\emph{Confusion Matrix})}
Para facilitar a análise de problemas \emph{multi-class}, criou-se a matriz de confusão. Ela consiste em uma matriz de números na qual as linhas representam as classificações realizadas pelo classificador e as colunas representam as classes originais das quais os objetos pertencem. A célula $a_{ij}$ da matriz possui um número que indica quantas vezes o classificador classificou objetos de tipo j como i.

É fácil de notar que uma matriz de confusão de um classificador perfeito possui vários números positivos na diagonal principal e zero em todas as outras células. Nota-se que qualquer numero maior que zero em uma célula que não pertence a diagonal principal representa um erro. Com a matriz de confusão fica fácil de visualizar os erros mais comuns feitos pelo classificador.

A Figura \ref{fig:confusion_matrix_example} mostra uma matriz de confusão de um classificador que deve classificar expressões faciais.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.9\textwidth]{confusion_matrix_example.png}
	\caption{Exemplo de Matriz de Confusão para um classificador retirado de \cite{bollano2007method}}
	\label{fig:confusion_matrix_example}
\end{figure}

A partir da matriz de confusão é fácil de determinar a precisão e a abrangência para cada classe i, conforme segue:

$precisao(i) = \frac{a_{ii}}{\sum_{j=1}^{L} a_{ij}} $

$abrangencia(i) = \frac{a_{ii}}{\sum_{j=1}^{L} a_{ji}} $

Ou seja, a precisão é um dos elementos da diagonal principal dividido pela soma de sua linha, enquanto que a abrangência é um dos elementos da diagonal principal dividido pela sua coluna.

Apesar de ter números específicos é muito bom pra analisar o quão bom um classificador é, é sempre importante observar a Matriz de Confusão pra detectar possíveis problemas e os lugares onda há erros mais comuns.

\subsubsection{Médias micro e macro (\emph{Micro and Macro Averaging})}
Conforme o descrito, pode-se calcular a precisão, a abrangência, o Fscore e a acurácia para cada classe individualmente. Entretanto seria interessante juntar todos estes dados em um único número. Para tal, faz-se uma média. Existem duas formas de realizar esta média, que são chamadas de micro e macro. 

A média micro consiste em somar os numeradores e denominadores das formulas das métricas supracitadas separadamente (na prática isto é equivalente a realizar uma média ponderada nos denominadores). Seguem as fórmulas de precisão, abrangência e acurácia micro.

$precisao_\mu = \frac{\sum_{i=1}^{L} {tp_i}}{\sum_{i=1}^{L} {tp_i + fp_i}}$

$abrangencia_\mu = \frac{\sum_{i=1}^{L} {tp_i}}{\sum_{i=1}^{L} {tp_i + fn_i}}$

Com uma matemática simples da pra concluir que a precisão micro é numericamente igual à abrangência micro (basta notar que o numerador dos dois é igual e o denominador dos dois representa a soma de todos os elementos da matriz de confusão, um através da soma das linhas e o outro através da soma das colunas).

A média macro consiste em somar os valores finais das métricas calculadas em cada classe e dividir pelo total de classes (uma média aritmética simples). Seguem as fórmulas de precisão e abrangência macro.

$precisao_M = \frac{\sum_{i=1}^{L} {\frac{tp_i}{tp_i + fp_i}}}{L}$

$abrangencia_M = \frac{\sum_{i=1}^{L} {\frac{tp_i}{tp_i + fn_i}}}{L}$

A intuição por trás dessas duas médias é que a média macro dá igual importância para todas as classes, enquanto que a média micro da mais importância para as classes mais comuns (no caso da abrangência) ou para as classes com mais objetos classificados nelas (no caso da precisão).

\subsubsection{Fscore}
Da mesma forma que foi definido o Fscore para o caso do classificador binário, é possível definir um Fscore macro e um Fscore micro (dependendo da utilização da precisão e abrangência macro ou micro).

$F_{\beta \mu} = \frac{(1+\beta^2).precisao_\mu .abrangencia_\mu}{\beta^2 precisao_\mu + abrangencia_\mu}$

$F_{\beta M} = \frac{(1+\beta^2).precisao_M .abrangencia_M}{\beta^2 precisao_M + abrangencia_M}$

Uma observação interessante é que, como a precisão micro e a abrangência micro são numericamente iguais, temos que o Fscore micro tambem terá este mesmo valor (independentemente de $\beta$). Para a média macro isto não acontece.

\subsubsection{Acurácia}

É possível encontrar na literatura duas definições diferentes de acurácia \cite{caballero2010sensitivity} \cite{sokolova2009systematic}. 

Em uma das definições, a acurácia de um classificador \emph{multi-class} é a média aritmética (média macro) das acurácias individuais de cada classe.

$accuracy = \frac{\sum_{i=1}^{L} {\frac{tp_i + tn_i}{tp_i + fn_i + fp_i + tn_i}}}{L}$

Na outra definição, a acurácia é a quantidade total de acertos (considerando todas as classes), dividida pela quantidade total de documentos.

\begin{equation}
accuracy = \frac{\sum_{i=1}^{L}{tp_i}}{N}
\label{eq:accuracy_multi_class}
\end{equation}

Considerando a matriz de confusão, temos que:

$accuracy = \frac{\sum_{i=1}^{L}{a_{ii}}}{\sum_{i=1}^{L}{\sum_{j=1}^{L}{a_{ij}}}} = \frac{\sum_{i=1}^{L}{a_{ii}}} {N}$

Neste trabalho será utilizada esta última definição por ser mais intuitiva e dar mais informação sobre a qualidade do classificador (observe que a primeira definição de acurácia tende a sempre resultar em valores altos para problemas com muitas classes, uma vez que a quantidade de verdadeiros negativos tende a ser alta para todas as classes).

Observa-se que a acurácia também é numericamente igual à abrangência e à precisão com média micro.

\subsubsection{Medida Kappa}
\label{sec:kappa}
Como já foi discutido previamente, a acurácia é uma medida muito ruim pois é possível desenvolver classificadores com comportamento trivial que possuem uma boa acurácia. Para resolver este problema, utilizam-se outras estatísticas em vez da acurácia que possuem mais valor semântico e dão mais informações sobre a performance do classificador. Já foram explicadas outras medidas que possuem mais informações (abrangência, precisão e Fscore micros e macros) e a matriz de confusão que permite a visualização de problemas recorrentes no classificador. Entretanto, para o problema \emph{multi-class} é comum utilizar uma outra métrica (chamada de Kappa) para avaliar a performance do classificador em questão.

A estatística Kappa é utilizada para comparar a Acurácia Observada com a Acurácia Esperada. Esta estatística é muito poderosa e permite a realização de comparações até mesmo entre classificadores diferentes \cite{kappa_statistic}. Ela utiliza a comparação em relação a um classificador aleatório de modo a tornar esta estatística mais confiável que uma simples acurácia.

A Acurácia Observada é a mesma acurácia que já foi definida previamente na equação \ref{eq:accuracy_multi_class}. Ela representa a quantidade de objetos classificados corretamente em relação ao total.

Define-se como Acurácia Esperada, a porcentagem esperada de acertos que um classificador aleatório teria se ele classificar os objetos com a mesma proporção entre as classes que o classificador original. 

Para se deduzir a fórmula da Acurácia Esperada primeiro calcula-se a quantidade de vezes que o classificador original escolheu a classe $c_i$. É fácil de ver que ele escolheu esta classe $tp_i + fp_i$ vezes. O valor esperado do número de acertos de um classificador aleatório que escolha a dada classe $tp_i + fp_i$ vezes é: 

$E[c_i] = (tp_i + fp_i)\frac{tp_i + fn_i}{N}$ 

Isto ocorre pois $\frac{tp_i + fn_i}{N}$ é a probabilidade de que um objeto aleatório pertença à classe $i$. A quantidade esperada para o número de acertos em todas as classes é de: 

$E[c_1,c_2,...,c_L] = \sum_{i=1}^{L} {(tp_i + fp_i)\frac{tp_i + fn_i}{N}}$

Como a Acurácia Esperada é a porcentagem de acertos desse classificador aleatório tem-se:

\begin{equation}
acuracia\_esperada = \frac{E[c_1,c_2,...,c_L]}{N} =  \frac{\sum_{i=1}^{L}{(tp_i + fp_i)(tp_i + fn_i)}}{N^2}
\label{eq:acuracia_esperada}
\end{equation}


Em termos da matriz de confusão, a acurácia esperada é a somatória dos produtos entre cada linha com a sua respectiva coluna (linha 1 com coluna 1, linha 2 com coluna 2 e assim por diante) dividida pela soma total de elementos ao quadrado.

A definição de Acurácia Esperada é importante pois um classificador de Acurácia Observada 80\% é muito mais impressionante se a Acurácia Esperada for de 1\% do que se ela for 79\%.

A estatística $Kappa$ indica a porcentagem de melhoria que o classificador tem em relação à Acurácia Esperada \cite{viera2005understanding}. Um classificador com Acurácia Observada idêntica à Acurácia Esperada possui $Kappa=0$, enquanto que um classificador com Acurácia Observada de 100\% possui $Kappa=1$. Valores intermediários do $Kappa$ indicam a melhoria do classificador em relação ao aleatório. Desta forma, a fórmula para o $Kappa$ é:

$Kappa = \frac{acuracia\_observada - acuracia\_esperada}{1 - acuracia\_esperada}$

A interpretação do $Kappa$ é interessante pois permite a comparação de diversos classificadores. Como pode ser visto na fórmula, a estatística $Kappa$ indica o ganho percentual entre um classificador randômico (que classifique com mesma proporção de classes que o classificador desenvolvido) e o classificador dado. Um $Kappa$ de 0\% significa que não há ganho enquanto que um $Kappa$ indica uma classificação perfeita. Landis e Koch fizeram um estudo dos diferentes valores de Kappa e chegaram à Tabela \ref{tab:kappa_table} abaixo de benchmarks para o $Kappa$ \cite{landis1977measurement}.


\begin{table}[tph]
	\begin{center}
		\begin{tabular}{c c}
			\hline
			$Kappa$ & Strength of Agreement \\
			\hline 
			<0.00 & Poor \\
			0.00-0.20 & Slight \\
			0.21-0.40 & Fair \\
			0.41-0.60 & Moderate \\
			0.61-0.80 & Substantial \\
			0.81-1.00 & Almost Perfect \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Nomeclatura para diferentes intervalos de $Kappa$ segundo os pesquisadores Landis e Koch}
	\label{tab:kappa_table}
\end{table}


Estas divisões foram arbitradas pelos autores supracitados e elas criam um benchmark útil para a verificação da qualidade de um classificador, definindo uma nomeclatura que pode ser utilizada pelo meio acadêmico. Obviamente, dependendo da aplicação pode ser necessário e desejável que se tenha um $Kappa$ de 0.9. Em outras aplicações um $Kappa$ de 0.4 pode já ser suficiente.

\section{Revisão Bibliográfica}
Muito já foi estudado no meio acadêmico sobre a utilização de Naïve Bayes para classificação de textos. McCallum e Nigam estudaram as diferentes modelagens possíveis para classificação de textos utilizando Naïve Bayes, explorando as diferenças entre modelos Multinomiais e de Bernoulli \cite{mccallum1998comparison}. Lewis e Ringuette estudam as diferenças entre a abordagem Bayesiana e a abordagem por árvores de decisão na classificação de textos \cite{lewis1994comparison}. Zaidi, Cerquides e Carman estudaram formas de aliviar a hipótese de independência condicional das Naïve Bayes, utilizando pesos para as features \cite{zaidi2013alleviating}. Lee, Dou e Gutierrez estudaram por que a métrica de Kullback-Leibler pode ser utilizada como peso \cite{weighted_naive_bayes}. Nirmala, Kumar, Velligiri, Patel e Mistry estudam vantagens e desvantagens de diferentes métodos de classificação de texto em redes sociais \cite{onlinesocialnetworks} \cite{patelreview}.

Muito pouco foi encontrado com relação à utilização de Redes Bayesiana em classificação de postagens de rede social. Este problema tem uma complexidade adicional pelo fato dos textos serem mais informais e com poucas palavras. Os poucos trabalhos encontrados fizeram muitas análises teóricas, porém sem muitos resultados práticos. Além de estudar os resultados numéricos do classificador desenvolvido, este trabalho explora a influência da inclusão de features não textuais (como o autor da postagem, o tipo de postagem, etc) na classificação.

